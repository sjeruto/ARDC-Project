# -*- coding: utf-8 -*-
"""NLP_ARDC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NdX4ZhH6daGlF7T59-1jCWJFysoBNDjs

#**News Tool**

Hi! Welcome to the news tool, set your query terms and parameters below (choose keywords and set parameters) and run the whole notebook, the go to the dashboard section for plots and resulting dataframe will all the articles collected.

##First steps.

Import Packages.
"""

import nltk
nltk.download('punkt')  
nltk.download('averaged_perceptron_tagger')  
nltk.download('maxent_ne_chunker') 
nltk.download('words')  
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd
nltk.download('omw-1.4')
from textblob import TextBlob
from dateutil import parser
import requests as r
import seaborn as sns
import matplotlib.pyplot as plt
import json
from datetime import date
from datetime import datetime
import spacy

"""Choose keywords and set parameters"""

guardian_api_key = '231ce917-65b5-4019-b365-c79f213379d1'
query_keywords = '"Rio Tinto"' #Delete double quotes if not an exact phrase query (free text)
query_fields = 'headline' #Other popular option body
from_date = '2000-01-01' #'yyyy,MM,dd'
to_date= '2022-12-31' #'yyyy,MM,dd'
page= '1' # Page number to extract articles from, can only do one page at a time
page_size = '100' # up to 200
prod_off = 'aus' #only news from the australia edition
order_by = 'newest' # relevance also available.

"""## Processing Data.

Call API and generate response with articles
"""

response = r.get(f'https://content.guardianapis.com/search?q={query_keywords}&query-fields={query_fields}&from-date={from_date}&to-date={to_date}&page={page}&page-size={page_size}&production-office={prod_off}&order-by={order_by}&api-key={guardian_api_key}')
art_response = response.json()

art_response

# Creating a loop to extract article headline.
art_title = []
for article in art_response["response"]["results"]:
    extract_title= article['webTitle']
    art_title.append(extract_title)

# Creating a loop to extract article content.
art_body = []
for article in art_response["response"]["results"]:
    url= (f'{article["apiUrl"]}?api-key={guardian_api_key}&show-fields=bodyText')
    article_content= r.get(url).json()
    body_text= article_content["response"]['content']['fields']['bodyText']
    art_body.append(body_text)

# Creating a loop to extract article date.
art_dates = []
for article in art_response["response"]["results"]:
    extract_dates= article['webPublicationDate']
    extract_dates = parser.parse(extract_dates)
    extract_dates = extract_dates.date()
    art_dates.append(extract_dates)

# Creating a loop to extract article section.
art_section = []
for article in art_response["response"]["results"]:
    extract_sect= article['sectionName']
    art_section.append(extract_sect)

# Creating a loop to extract url.
art_url = []
for article in art_response["response"]["results"]:
    extract_url= article['webUrl']
    art_url.append(extract_url)

art_body_date = pd.DataFrame(
    {'date': art_dates,
     'headline': art_title,
     'article': art_body,
     'section': art_section,
     'url': art_url
     })

art_body_date['year_month'] = pd.to_datetime(art_body_date['date']).dt.to_period('M')

plot_overtime = art_body_date['year_month'].value_counts().sort_index().plot(kind='line')

"""Creating a loop to extract Named entities recognized."""

art_body_date['chunks'] = art_body_date.article.apply(lambda x: nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(x))))
art_ent = []
for i, v in art_body_date['chunks'].iteritems():
  temp_list = []
  for chunk in v:
    if hasattr(chunk,'label'):
      temp_list.append(str(chunk))
  art_ent.append(temp_list)

art_body_date_ent = pd.DataFrame(
    {'date': art_dates,
     'headline': art_title,
     'article': art_body,
     'section': art_section,
     'url': art_url,
     'entities': art_ent
     })

"""Pre-processing"""

art_clean = art_body_date_ent

art_clean['article'] = art_clean['article'].astype(str).str.lower()
art_clean['article'] = art_clean['article'].str.replace(r'[^\w\s]+', '')
art_clean['article'] = art_clean['article'].str.replace('\d+', '')
from nltk.tokenize import RegexpTokenizer
regexp = RegexpTokenizer('\w+')
art_clean['text_token']=art_clean['article'].apply(regexp.tokenize)
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = nltk.corpus.stopwords.words("english")
newStopWords = ['rio','tinto','said','would','mining','company']
stopwords.extend(newStopWords)
art_clean['text_token'] = art_clean['text_token'].apply(lambda x: [item for item in x if item not in stopwords])
art_clean['text_string'] = art_clean['text_token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))

all_words = ' '.join([word for word in art_clean['text_string']])
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize

words = nltk.word_tokenize(all_words)
fd = FreqDist(words)
top_10 = fd.most_common(10)
fdist = pd.Series(dict(top_10))
import seaborn as sns
sns.set_theme(style="ticks")

sns.barplot(y=fdist.index, x=fdist.values, color='blue')

"""Sentiment Analysis"""

analyzer = SentimentIntensityAnalyzer()

art_body_date_ent['polarity'] = art_body_date_ent['headline'].apply(lambda x: analyzer.polarity_scores(x))

# Change data structure
art_body_date_ent = pd.concat(
    [art_body_date_ent.drop(['polarity'], axis=1),
     art_body_date_ent['polarity'].apply(pd.Series)], axis=1)

# Create new variable with sentiment "neutral," "positive" and "negative"
art_body_date_ent['sentiment'] = art_body_date_ent['compound'].apply(lambda x: 'positive' if x >0 else 'neutral' if x==0 else 'negative')

"""#DASHBOARD

*Results*
"""

print(f'your query for news articles from {query_keywords} in the production office {prod_off} threw {arts} results')

"""Articles Over time."""

plot_overtime = art_body_date['year_month'].value_counts().sort_index().plot(kind='line')

"""Sentiment by Article"""

arts = art_response["response"]['total']

test2 = sns.countplot(y='sentiment',
             data=art_body_date_ent,
              palette=['#b2d8d8',"#008080", '#db3d13']
             ).set(title='Sentiment analysis on headlines')

"""Sentiment over time"""

g = sns.lineplot(x='date', y='compound', data=art_body_date_ent)

g.set(title='Sentiment of Articles according to headline')
g.set(xlabel="Time")
g.set(ylabel="Sentiment")
g.tick_params(bottom=False)

g.axhline(0, ls='--', c = 'grey')

"""Sentiment Boxplot (Distribution)"""

g = sns.lineplot(x='date', y='compound', data=art_body_date_ent)
g.set(xticklabels=[])
g.set(title='Sentiment of Articles')
g.set(xlabel="Time")
g.set(ylabel="Sentiment")
g.tick_params(bottom=False)
g.axhline(0, ls='--', c = 'grey')

test1 = sns.boxplot(y='compound',
            x='sentiment',
            palette=['#b2d8d8',"#008080", '#db3d13'],
            data=art_body_date_ent)

Most common words

sns.barplot(y=fdist.index, x=fdist.values, color='blue')

"""Articles Dataframe (Explore)"""

art_body_date_ent